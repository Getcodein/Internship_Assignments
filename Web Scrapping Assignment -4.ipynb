{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299234af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 4\n",
    "#----------------question1----------------------------\n",
    "# Importing Libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "time.sleep(2)\n",
    "\n",
    "# clicking search button using xpath function\n",
    "#search_btn=driver.find_element(By.XPATH,'//input[@id=\"nav-search-submit-button\"]')\n",
    "#search_btn.click()\n",
    "\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "View=[]\n",
    "\n",
    "rank_tags=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody[1]/tr/td[1]')\n",
    "name_tags=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody[1]/tr/td[2]')\n",
    "Artist_tags=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody[1]/tr/td[3]')\n",
    "upload_tags=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody[1]/tr/td[4]')\n",
    "views_tags=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody[1]/tr/td[5]')\n",
    "\n",
    "print(len(rank_tags),len(name_tags),len(Artist_tags),len(upload_tags),len(views_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rank_tags),len(name_tags),len(Artist_tags),len(upload_tags),len(views_tags))\n",
    "\n",
    "rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "View=[]\n",
    "\n",
    "for k in rank_tags[:30]:\n",
    "    m=k.text\n",
    "    rank.append(m)\n",
    "\n",
    "for k1 in name_tags[:30]:\n",
    "    m1=k1.text\n",
    "    Name.append(m1)\n",
    "    \n",
    "\n",
    "for k2 in Artist_tags[:30]:\n",
    "    m2=k2.text\n",
    "    Artist.append(m2)\n",
    "    \n",
    "\n",
    "for k3 in upload_tags[:30]:\n",
    "    m3=k3.text\n",
    "    Upload_date.append(m3)\n",
    "\n",
    "for k4 in views_tags[:30]:\n",
    "    m4=k4.text\n",
    "    View.append(m4)\n",
    "\n",
    "print(len(rank),len(Name),len(Artist),len(Upload_date),len(Views))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc28a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Rank']=rank\n",
    "shop['Name']=Name\n",
    "shop['Artist']=Artist\n",
    "shop['Upload_date']=View\n",
    "shop['Views']=Upload_date\n",
    "\n",
    "\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 2----------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "time.sleep(7)\n",
    "\n",
    "try:\n",
    "# clicking search button using xpath function\n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a')\n",
    "    search_btn.click()\n",
    "    time.sleep(7)\n",
    "\n",
    "except  ElementNotInteractableException as e:\n",
    "    print(\"hi\")\n",
    "    driver.get(\"https://www.bcci.tv/international/fixtures\")\n",
    "    \n",
    "\n",
    "Match_title=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "\n",
    "Match_tags=driver.find_elements(By.XPATH,'//div[@class=\"fixture-card-top\"]/h5[2]')\n",
    "Series_tags=driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]/span[1]')\n",
    "Place_tags=driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]/span[2]')\n",
    "Date_tags=driver.find_elements(By.XPATH,'//div[@class=\"match-card-left match-schedule\"]/h5')\n",
    "Time_tags=driver.find_elements(By.XPATH,'//div[@class=\"match-card-right match-schedule \"]/h5')\n",
    "\n",
    "print(len(Match_tags),len(Series_tags),len(Place_tags),len(Date_tags),len(Time_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4dbf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Match_title=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "\n",
    "for k in Match_tags:\n",
    "    m=k.text\n",
    "    Match_title.append(m)\n",
    "\n",
    "for k1 in Series_tags:\n",
    "    m1=k1.text\n",
    "    Series.append(m1)\n",
    "    \n",
    "\n",
    "for k2 in Place_tags:\n",
    "    m2=k2.text\n",
    "    Place.append(m2)\n",
    "    \n",
    "\n",
    "for k3 in Date_tags:\n",
    "    m3=k3.text\n",
    "    Date.append(m3)\n",
    "\n",
    "for k4 in Time_tags:\n",
    "    m4=k4.text\n",
    "    Time.append(m4)\n",
    "\n",
    "\n",
    "print(len(Match_title),len(Series),len(Place),len(Date),len(Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165db222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Match_title']=Match_title\n",
    "shop['Series']=Series\n",
    "shop['Place']=Place\n",
    "shop['Date']=Date\n",
    "shop['Time']=Time\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 3----------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "time.sleep(7)\n",
    "\n",
    "try:\n",
    "# clicking search button using xpath function\n",
    "    # clicking search button using xpath function\n",
    "     # clicking search button using xpath function\n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[2]/div[1]/div/ul[1]/li[3]/a')\n",
    "    search_btn.click()\n",
    "    time.sleep(7)\n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[1]/a/strong')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "except  ElementNotInteractableException as e:\n",
    "    driver.get(\"https://www.guru99.com/exception-handling-selenium.html\")\n",
    "    \n",
    "    \n",
    "name_tags=driver.find_elements(By.XPATH,'//div[@class=\"entry-content single-content\"]/p')\n",
    "description_tags=driver.find_elements(By.XPATH,'//div[@class=\"entry-content single-content\"]/p')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(name_tags),len(description_tags))\n",
    "\n",
    "Name=[]\n",
    "descr=[]\n",
    "\n",
    "\n",
    "for k in name_tags[:41]:\n",
    "    m=k.text.split(\":\")[0]\n",
    "    Name.append(m)\n",
    "\n",
    "for k1 in description_tags[:41]:\n",
    "    m1=k1.text.split(\":\")[1]\n",
    "    descr.append(m1)\n",
    "    \n",
    "\n",
    "print(len(Name),len(descr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ecef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Name']=Name\n",
    "shop['descr']=descr\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 4----------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.statisticstimes.com/\")\n",
    "time.sleep(7)\n",
    "\n",
    "try:\n",
    "# clicking search button using xpath function\n",
    "   \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]/button')\n",
    "    search_btn.click()\n",
    "    time.sleep(7)\n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "except  NoSuchElementException as e:\n",
    "    print(\"hi\")\n",
    "    driver.get(\"https://www.statisticstimes.com/economy/india/indian-states-gdp.php\")\n",
    "    \n",
    "    \n",
    "name_tags=driver.find_elements(By.XPATH,'//div[@id=\"main\"]/div[5]/div/div/table/tbody[1]/tr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(name_tags))\n",
    "\n",
    "\n",
    "Rank=[]\n",
    "State=[]\n",
    "GSDP1819=[]\n",
    "GSDP1920=[]\n",
    "Share=[]\n",
    "GDP=[]\n",
    "\n",
    "\n",
    "for k in name_tags:\n",
    "    m=k.text.split(\" \")[0]\n",
    "    Rank.append(m)\n",
    "    m2=k.text.split(\" \")[1]\n",
    "    State.append(m2)\n",
    "    m3=k.text.split(\" \")[2]\n",
    "    GSDP1819.append(m3)\n",
    "    m4=k.text.split(\" \")[3]\n",
    "    GSDP1920.append(m4)\n",
    "    m5=k.text.split(\" \")[4]\n",
    "    Share.append(m5)\n",
    "    m6=k.text.split(\" \")[5]\n",
    "    GDP.append(m6)\n",
    "    \n",
    "print(len(Rank),len(State),len(GSDP1819),len(GSDP1920),len(Share),len(GDP))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e97afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSDP1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Rank']=Rank\n",
    "shop['State']=State\n",
    "shop['GSDP:18-19']=GSDP1819\n",
    "shop['GSDP:19-20']=GSDP1920\n",
    "shop['Share']=Share\n",
    "shop['GDP']=GDP\n",
    "\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 5----------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.statisticstimes.com/\")\n",
    "time.sleep(7)\n",
    "\n",
    "try:\n",
    "# clicking search button using xpath function\n",
    "   \n",
    "    search_btn=driver.find_element(By.XPATH,' /html/body/div[1]/div[1]/header/div/div[1]/div[2]/button/span/span')\n",
    "    search_btn.click()\n",
    "    time.sleep(7)\n",
    "    \n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[1]/button')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[1]/div/ul[2]/li[2]/a')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    srch_jb = driver.find_element(By.XPATH,'//input[@class=\"text\"]')\n",
    "    srch_jb.send_keys(\"repo\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "\n",
    "except  NoSuchElementException as e:\n",
    "    print(\"hi\")\n",
    "    driver.get(\"https://github.com/search?q=repo&type=repositories\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "Repo_tags=driver.find_elements(By.XPATH,'//div[@class=\"mt-n1 flex-auto\"]/div[1]/div/a')\n",
    "Repodes_tags=driver.find_elements(By.XPATH,'//div[@class=\"mt-n1 flex-auto\"]/p[1]')\n",
    "cont_tags=driver.find_elements(By.XPATH,'//div[@class=\"d-flex flex-wrap text-small color-fg-muted\"]/div[1]/a')\n",
    "lang_tags=driver.find_elements(By.XPATH,'//div[@class=\"d-flex flex-wrap text-small color-fg-muted\"]/div[2]')\n",
    "\n",
    "\n",
    "print(len(Repo_tags),len(Repodes_tags),len(cont_tags),len(lang_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Repo_tags))\n",
    "\n",
    "\n",
    "Repository_title=[]\n",
    "Repository_description=[]\n",
    "Contributors_count=[]\n",
    "Language=[]\n",
    "\n",
    "for k in Repo_tags:\n",
    "    m=k.text\n",
    "    Repository_title.append(m)\n",
    "\n",
    "for k1 in Repodes_tags:\n",
    "    m1=k1.text\n",
    "    Repository_description.append(m1)\n",
    "    \n",
    "\n",
    "for k2 in cont_tags:\n",
    "    m2=k2.text\n",
    "    Contributors_count.append(m2)\n",
    "    \n",
    "\n",
    "for k3 in lang_tags:\n",
    "    m3=k3.text\n",
    "    Language.append(m3)\n",
    "    \n",
    "    \n",
    "print(len(Repository_title),len(Repository_description),len(Contributors_count),len(Language))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ef152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Repository_title']=Repository_title\n",
    "shop['Repository_description']=Repository_description\n",
    "shop['Contributors_count']=Contributors_count\n",
    "shop['Language']=Language\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 6----------------------------\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.billboard.com/\")\n",
    "time.sleep(7)\n",
    "\n",
    "try:\n",
    "# clicking search button using xpath function\n",
    "   \n",
    "    search_btn=driver.find_element(By.XPATH,' /html/body/div[1]/div[1]/header/div/div[1]/div[2]/button/span/span')\n",
    "    search_btn.click()\n",
    "    time.sleep(7)\n",
    "    \n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[1]/button')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[1]/div/ul[2]/li[2]/a')\n",
    "    search_btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    srch_jb = driver.find_element(By.XPATH,'//input[@class=\"text\"]')\n",
    "    srch_jb.send_keys(\"repo\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "\n",
    "except  NoSuchElementException as e:\n",
    "    print(\"hi\")\n",
    "    driver.get(\"https://www.billboard.com/charts/hot-100/\")\n",
    "    \n",
    "    \n",
    "\n",
    "songs_tags=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li/h3')\n",
    "artist_tags=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul[1]/li[1]/span[1]')\n",
    "last_tags=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul[1]/li[4]/span[1]')\n",
    "peak_tags=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul[1]/li[5]/span[1]')\n",
    "weeksboard=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul[1]/li[6]/span[1]')\n",
    "\n",
    "print(len(songs_tags),len(artist_tags),len(last_tags),len(peak_tags),len(weeksboard))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f07331",
   "metadata": {},
   "outputs": [],
   "source": [
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Lastweekrank=[]\n",
    "Peakrank=[]\n",
    "Wks=[]\n",
    "\n",
    "for k4 in weeksboard:\n",
    "    Wks.append(k4.text)\n",
    "    \n",
    "for k in songs_tags:\n",
    "    m=k.text\n",
    "    Song_name.append(m)\n",
    "\n",
    "for k1 in artist_tags:\n",
    "    m1=k1.text\n",
    "    Artist_name.append(m1)\n",
    "    \n",
    "\n",
    "for k2 in last_tags:\n",
    "    m2=k2.text\n",
    "    Lastweekrank.append(m2)\n",
    "    \n",
    "\n",
    "for k3 in peak_tags:\n",
    "    m3=k3.text\n",
    "    Peakrank.append(m3)\n",
    "\n",
    "\n",
    "\n",
    "print(len(Song_name),len(Artist_name),len(Lastweekrank),len(Peakrank),len(Wks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Song name']=Song_name\n",
    "shop['Artist name']=Artist_name\n",
    "shop['Last week rank']=Lastweekrank\n",
    "shop['Peak rank']=Peakrank\n",
    "shop['Weeks on board']=Wks\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fdbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------answer 7----------------------------------------------------\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from selenium.common.exceptions import staleElementRefereneceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "#connecting to webdriver\n",
    "try:\n",
    "    driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "    url=\"https://www.naukri.com/\"\n",
    "    driver.get(\"https://www.naukri.com/hr-recruiters-consultants\")\n",
    "\n",
    "    # finding element for job search bar\n",
    "    srch_jb = driver.find_element(By.CLASS_NAME,\"sugInp\")\n",
    "    srch_jb.send_keys('Data Science')\n",
    "\n",
    "    # clicking search button using xpath function\n",
    "    search_btn=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/button\")\n",
    "    search_btn.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "except  NoSuchElementException as e:\n",
    "    print(\"hi\")\n",
    "    driver.get(\"https://www.naukri.com/data-science-recruiters\")\n",
    "    \n",
    "\n",
    "job_titles=[]\n",
    "desgination=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "locations_tags=[]\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"ellipsis\"]/span[1]')\n",
    "for i in title_tags:\n",
    "    t=i.text\n",
    "    job_titles.append(t)       \n",
    "\n",
    "des_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis clr\"]')\n",
    "desgination=[]\n",
    "for i in des_tags:\n",
    "    d=i.text\n",
    "    desgination.append(d)  \n",
    "    \n",
    "\n",
    "locations_tags=driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]/span[1]')\n",
    "job_location=[]\n",
    "for i in locations_tags:\n",
    "    m=i.text\n",
    "    job_location.append(m)     \n",
    "\n",
    "\n",
    "company_name=[]\n",
    "company_tags=driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]/a[2]')\n",
    "for i in company_tags:\n",
    "    n=i.text\n",
    "    company_name.append(n)\n",
    "\n",
    "experience_tags=driver.find_elements(By.XPATH,'//div[@class=\"hireSec highlightable\"]')\n",
    "experience_tags\n",
    "for i in experience_tags:\n",
    "    l=i.text\n",
    "    experience_required.append(l)\n",
    "\n",
    "# creating dataframe\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Name']=job_titles\n",
    "jobs['Designation']=desgination\n",
    "jobs['company']=company_name\n",
    "jobs['Skills they hire for']=experience_required\n",
    "jobs['location']=job_location\n",
    "\n",
    "print(jobs)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2301f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 8---------------------------\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "time.sleep(7)\n",
    "\n",
    "    \n",
    "\n",
    "book_tags=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[2]')\n",
    "a_tags=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[3]')\n",
    "v_tags=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[4]')\n",
    "p_tags=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[5]')\n",
    "genere_tags=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[6]')\n",
    "\n",
    "print(len(book_tags),len(a_tags),len(v_tags),len(p_tags),len(genere_tags))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bookname=[]\n",
    "Authorname=[]\n",
    "Volumessold=[]\n",
    "Publisher=[]\n",
    "Genre=[]\n",
    "\n",
    "for k4 in book_tags:\n",
    "    Bookname.append(k4.text)\n",
    "    \n",
    "for k in a_tags:\n",
    "    m=k.text\n",
    "    Authorname.append(m)\n",
    "\n",
    "for k1 in v_tags:\n",
    "    m1=k1.text\n",
    "    Volumessold.append(m1)\n",
    "    \n",
    "\n",
    "for k2 in p_tags:\n",
    "    m2=k2.text\n",
    "    Publisher.append(m2)\n",
    "    \n",
    "\n",
    "for k3 in genere_tags:\n",
    "    m3=k3.text\n",
    "    Genre.append(m3)\n",
    "\n",
    "print(len(Bookname),len(Authorname),len(Volumessold),len(Publisher),len(Genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Book_name']=Bookname\n",
    "shop['Author_name']=Authorname\n",
    "shop['Volumes_sold']=Volumessold\n",
    "shop['Publisher']=Publisher\n",
    "shop['Genre']=Genre\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------answer 9------------------------------------------\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#connecting to webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "time.sleep(7)\n",
    "\n",
    "    \n",
    "\n",
    "n_tags=driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/h3[1]/a')\n",
    "y_tags=driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/h3[1]/span[2]')\n",
    "g_tags=driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p[1]/span[5]')\n",
    "r_tags=driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p[1]/span[3]')\n",
    "rat_tags=driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/div[1]/div[1]/span[2]')\n",
    "v_tags=driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p[4]/span[2]')\n",
    "\n",
    "print(len(n_tags),len(y_tags),len(g_tags),len(r_tags),len(rat_tags),len(v_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=[]\n",
    "Year=[]\n",
    "Genre=[]\n",
    "Run=[]\n",
    "Ratings=[]\n",
    "Votes=[]\n",
    "\n",
    "\n",
    "\n",
    "for k4 in n_tags:\n",
    "    Name.append(k4.text)\n",
    "    \n",
    "for k in y_tags:\n",
    "    m=k.text\n",
    "    Year.append(m)\n",
    "\n",
    "for k1 in g_tags:\n",
    "    m1=k1.text\n",
    "    Genre.append(m1)\n",
    "    \n",
    "\n",
    "for k2 in r_tags:\n",
    "    m2=k2.text\n",
    "    Run.append(m2)\n",
    "    \n",
    "\n",
    "for k3 in rat_tags:\n",
    "    m3=k3.text\n",
    "    Ratings.append(m3)\n",
    "\n",
    "for k5 in v_tags:\n",
    "    Votes.append(k5.text)\n",
    "\n",
    "print(len(Name),len(Year),len(Genre),len(Run),len(Ratings),len(Votes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Name']=Name\n",
    "shop['Year_span']=Year\n",
    "shop['Genre']=Genre\n",
    "shop['Run time']=Run\n",
    "shop['Ratings']=Ratings\n",
    "shop['Votes']=Votes\n",
    "\n",
    "shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2919c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------answer 10----------------------------------------------------\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from selenium.common.exceptions import staleElementRefereneceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "#connecting to webdriver\n",
    "try:\n",
    "    driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "    url=\"https://www.naukri.com/\"\n",
    "    driver.get(\"https://archive.ics.uci.edu/ml/index.php\")\n",
    "\n",
    "    # clicking search button using xpath function\n",
    "    search_btn=driver.find_element(By.XPATH,\"/html/body/table[1]/tbody/tr/td[2]/span[2]/a\")\n",
    "    search_btn.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "except  NoSuchElementException as e:\n",
    "    print(\"hi\")\n",
    "    driver.get(\"https://archive.ics.uci.edu/ml/datasets.php\")\n",
    "    \n",
    "    \n",
    "Datasetname=[]\n",
    "Datatype=[]\n",
    "Task=[]\n",
    "Attributetype=[]\n",
    "Noofinstance=[]\n",
    "Noofattribute=[]\n",
    "Year=[]\n",
    "\n",
    "n_tags=driver.find_elements(By.XPATH,'//table[@cellpadding=\"3\"]/tbody/tr/td[2]/table[2]/tbody/tr')\n",
    "\n",
    "\n",
    "print(n_tags)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c17f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(n_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasetname=[]\n",
    "Datatype=[]\n",
    "Task=[]\n",
    "Attributetype=[]\n",
    "Noofinstances=[]\n",
    "Noofattribute=[]\n",
    "Year=[]\n",
    "\n",
    "for k in n_tags[1:10]:\n",
    "    m=k.text.split(\"\\n\")[0]\n",
    "    Datasetname.append(m)\n",
    "\n",
    "for k in n_tags[1:10]:\n",
    "    m1=k.text.split(\"\\n\")[1]\n",
    "    Datatype.append(m1)\n",
    "    \n",
    "for k in n_tags[1:10]:\n",
    "    m2=k.text.split(\"\\n\")[2]\n",
    "    Task.append(m2)\n",
    "    \n",
    "for k in n_tags[1:10]:\n",
    "    m3=k.text.split(\"\\n\")[3]\n",
    "    Attributetype.append(m3)\n",
    "    \n",
    "for k in n_tags[1:10]:\n",
    "    m4=k.text.split(\"\\n\")[4]\n",
    "    Noofinstances.append(m4)\n",
    "    \n",
    "for k in n_tags[1:10]:\n",
    "    m5=k.text.split(\"\\n\")[5]\n",
    "    Noofattribute.append(m5)\n",
    "    \n",
    "for k in n_tags[1:10]:\n",
    "    m6=k.text.split(\"\\n\")[6]\n",
    "    Year.append(m6)\n",
    "\n",
    "    \n",
    "print(len(Datasetname),len(Datatype),len(Task),len(Attributetype),len(Noofinstances),len(Noofattribute),len(Year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a829b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------creating data frame -------------------------\n",
    "shop=pd.DataFrame({})\n",
    "shop['Name']=Name\n",
    "shop['Year_span']=Year\n",
    "shop['Genre']=Genre\n",
    "shop['Run time']=Run\n",
    "shop['Ratings']=Ratings\n",
    "shop['Votes']=Votes\n",
    "\n",
    "shop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
