{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d2b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shilp\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shilp\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "<h2>Navigation menu</h2>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "#answer 1\n",
    "\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    header_name=[]\n",
    "    hd=['h1','h2','h3','h4','h5','h6']\n",
    "    for i in soup.find_all(hd):\n",
    "       # header_name.append(i)\n",
    "         print(i,sep='\\n\\n')\n",
    "\n",
    "#calling function         \n",
    "url=\"https://en.wikipedia.org/wiki/Main_Page\"    \n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7f8b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    movies_name movie_rating movie_year\n",
      "0             1. The Shawshank Redemption(1994)          9.2     (1994)\n",
      "1                        2. The Godfather(1972)          9.2     (1972)\n",
      "2                      3. The Dark Knight(2008)          9.0     (2008)\n",
      "3                4. The Godfather Part II(1974)          9.0     (1974)\n",
      "4                         5. 12 Angry Men(1957)          9.0     (1957)\n",
      "..                                          ...          ...        ...\n",
      "95                       96. Citizen Kane(1941)          8.3     (1941)\n",
      "96  97. M - Eine Stadt sucht einen Mörder(1931)          8.3     (1931)\n",
      "97                 98. Lawrence of Arabia(1962)          8.3     (1962)\n",
      "98                 99. North by Northwest(1959)          8.2     (1959)\n",
      "99                           100. Vertigo(1958)          8.2     (1958)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#answer 2\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "#extracting movie names\n",
    "    mv_li=[]\n",
    "    movie_names=soup.find_all('td',class_='titleColumn')\n",
    "    for i in movie_names:\n",
    "        mv_li.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "        #mv_li.replace(\"      \",\"\")\n",
    "#extracting movie ratings   \n",
    "    movie_rating=soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "    mv_rt=[]\n",
    "    for j in movie_rating:\n",
    "        mv_rt.append(j.get_text().replace(\"\\n\",\"\").strip())\n",
    "#extracting movie year      \n",
    "    movie_year=soup.find_all('span',class_='secondaryInfo')\n",
    "    mv_yr=[]\n",
    "    for k in movie_year:\n",
    "        mv_yr.append(k.get_text().replace(\"\\n\",\"\").strip())\n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"movies_name\"]=mv_li\n",
    "    df[\"movie_rating\"]=mv_rt\n",
    "    df[\"movie_year\"]=mv_yr\n",
    "#print top 100 movies list\n",
    "    print(df.iloc[0:100,:])\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.imdb.com/chart/top/?sort=ir,desc&mode=simple&page=1\"    \n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba65a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     movies_name movie_rating movie_year\n",
      "0   1. Ramayana: The Legend of Prince Rama(1993)          8.5     (1993)\n",
      "1            2. Rocketry: The Nambi Effect(2022)          8.4     (2022)\n",
      "2                               3. Golmaal(1979)          8.4     (1979)\n",
      "3                           4. 777 Charlie(2022)          8.4     (2022)\n",
      "4                               5. Nayakan(1987)          8.4     (1987)\n",
      "..                                           ...          ...        ...\n",
      "95                      96. Kaakkaa Muttai(2014)          8.0     (2014)\n",
      "96                         97. Ustad Hotel(2012)          8.0     (2012)\n",
      "97            98. Theeran Adhigaaram Ondru(2017)          8.0     (2017)\n",
      "98                              99. Angoor(1982)          8.0     (1982)\n",
      "99                    100. Rang De Basanti(2006)          8.0     (2006)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# answer 3\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "#extracting movie names\n",
    "    mv_li=[]\n",
    "    movie_names=soup.find_all('td',class_='titleColumn')\n",
    "    for i in movie_names:\n",
    "        mv_li.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "        #mv_li.replace(\"      \",\"\")\n",
    "#extracting movie ratings   \n",
    "    movie_rating=soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "    mv_rt=[]\n",
    "    for j in movie_rating:\n",
    "        mv_rt.append(j.get_text().replace(\"\\n\",\"\").strip())\n",
    "#extracting movie year      \n",
    "    movie_year=soup.find_all('span',class_='secondaryInfo')\n",
    "    mv_yr=[]\n",
    "    for k in movie_year:\n",
    "        mv_yr.append(k.get_text().replace(\"\\n\",\"\").strip())\n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"movies_name\"]=mv_li\n",
    "    df[\"movie_rating\"]=mv_rt\n",
    "    df[\"movie_year\"]=mv_yr\n",
    "#print top 100 movies list\n",
    "    print(df.iloc[0:100,:])\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.imdb.com/india/top-rated-indian-movies/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=461131e5-5af0-4e50-bee2-223fad1e00ca&pf_rd_r=W8XT86ESC3P8HWH526R5&pf_rd_s=center-1&pf_rd_t=60601&pf_rd_i=india.toprated&ref_=fea_india_ss_toprated_india_tr_india250_sm\"    \n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88282e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Name  \\\n",
      "0           Shri Ram Nath Kovind (birth - 1945)   \n",
      "1             Shri Pranab Mukherjee (1935-2020)   \n",
      "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
      "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
      "4            Shri K. R. Narayanan (1920 - 2005)   \n",
      "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
      "6               Shri R Venkataraman (1910-2009)   \n",
      "7                  Giani Zail Singh (1916-1994)   \n",
      "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
      "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
      "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
      "11                 Dr. Zakir Husain (1897-1969)   \n",
      "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
      "13             Dr. Rajendra Prasad (1884-1963)    \n",
      "\n",
      "                                       Term of office  \n",
      "0     Term of Office: 25 July, 2017 to 25 July, 2022   \n",
      "1     Term of Office: 25 July, 2012 to 25 July, 2017   \n",
      "2     Term of Office: 25 July, 2007 to 25 July, 2012   \n",
      "3     Term of Office: 25 July, 2002 to 25 July, 2007   \n",
      "4     Term of Office: 25 July, 1997 to 25 July, 2002   \n",
      "5     Term of Office: 25 July, 1992 to 25 July, 1997   \n",
      "6     Term of Office: 25 July, 1987 to 25 July, 1992   \n",
      "7     Term of Office: 25 July, 1982 to 25 July, 1987   \n",
      "8     Term of Office: 25 July, 1977 to 25 July, 1982   \n",
      "9   Term of Office: 24 August, 1974 to 11 February...  \n",
      "10  Term of Office: 3 May, 1969 to 20 July, 1969 a...  \n",
      "11        Term of Office: 13 May, 1967 to 3 May, 1969  \n",
      "12       Term of Office: 13 May, 1962 to 13 May, 1967  \n",
      "13   Term of Office: 26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "# answer 4\n",
    "\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    #print(soup.prettify())\n",
    "#extracting president names\n",
    "    pr_name=[]\n",
    "    new_pn=[]\n",
    "    p_names=soup.find_all('div',class_='presidentListing')\n",
    "    p_names=soup.select(\"h3\")\n",
    "    #print(p_names)\n",
    "            \n",
    "    for i in p_names:\n",
    "        pr_name.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "                \n",
    "#extracting president term of office    \n",
    "    pr_off=[]\n",
    "    p_term=soup.find_all(\"div\",class_='presidentListing')\n",
    "    #p_term=p_term.select(\"p\")\n",
    "    #print(p_term)      \n",
    "    for i in p_term:\n",
    "        pr_off.append(i.get_text().split(\"\\n\")[2])\n",
    "   \n",
    "    \n",
    "\n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"Name\"]=pr_name\n",
    "    df[\"Term of office\"]=pr_off\n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a285202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     team_names matches points  \\\n",
      "0   New Zealand      23  2,670   \n",
      "1       England      30  3,400   \n",
      "2     Australia      32  3,572   \n",
      "3         India      35  3,866   \n",
      "4      Pakistan      22  2,354   \n",
      "5  South Africa      24  2,392   \n",
      "6    Bangladesh      30  2,753   \n",
      "7     Sri Lanka      30  2,677   \n",
      "8   Afghanistan      19  1,380   \n",
      "9   West Indies      41  2,902   \n",
      "\n",
      "                                             ratings  \n",
      "0  \\n                            116\\n           ...  \n",
      "1                                                113  \n",
      "2                                                112  \n",
      "3                                                110  \n",
      "4                                                107  \n",
      "5                                                100  \n",
      "6                                                 92  \n",
      "7                                                 89  \n",
      "8                                                 73  \n",
      "9                                                 71  \n"
     ]
    }
   ],
   "source": [
    "# answer 5 part A:\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "#extracting team names\n",
    "    mv_li=[]\n",
    "    team_names=soup.find_all('span',class_='u-hide-phablet')\n",
    "    for i in team_names:\n",
    "        mv_li.append(i.get_text())\n",
    "        \n",
    "    \n",
    "#extracting matches counts and points\n",
    "    mat_li=[]\n",
    "    mat_li2=[]\n",
    "    n=[]\n",
    "    p=[]\n",
    "    first_match=[]\n",
    "    first_match1=soup.find('td',class_='rankings-block__banner--matches')\n",
    "    first_match1=first_match1.get_text()\n",
    "    first_point1=soup.find('td',class_='rankings-block__banner--points')\n",
    "    first_point1=first_point1.get_text()\n",
    "    \n",
    "    mat_count=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    \n",
    "    mat_li.append(first_match1)\n",
    "    mat_li.append(first_point1)\n",
    "    \n",
    "    for i in mat_count:\n",
    "        mat_li.append(i.get_text())\n",
    "        \n",
    " \n",
    "   #n for storing first and all rest of the matches counts \n",
    "    k=0\n",
    "    for k in range(0,len(mat_li),2):\n",
    "        n.append(mat_li[k])\n",
    "\n",
    "        \n",
    "  #p for storing first and all rest of the matches points \n",
    "    y=0\n",
    "    for y in range(1,len(mat_li),2):\n",
    "        p.append(mat_li[y])\n",
    "    \n",
    "\n",
    "#extracting rating\n",
    "    rat_li=[]\n",
    "    \n",
    "    first_count1=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    first_count1=first_count1.get_text()\n",
    "    rat_count=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    \n",
    "    rat_li.append(first_count1)\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "        \n",
    "        \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"team_names\"]=mv_li\n",
    "    df[\"matches\"]=n\n",
    "    df[\"points\"]=p\n",
    "    df[\"ratings\"]=rat_li\n",
    "#print top 100 movies list\n",
    "    print(df.iloc[0:10,:])\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25415607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Top 10 batsmen list----------\n",
      "                  Player team ratings\n",
      "0             Babar Azam  PAK     890\n",
      "1            Imam-ul-Haq  PAK     779\n",
      "2  Rassie van der Dussen   SA     766\n",
      "3        Quinton de Kock   SA     759\n",
      "4           David Warner  AUS     747\n",
      "5            Steve Smith  AUS     719\n",
      "6         Jonny Bairstow  ENG     710\n",
      "7            Virat Kohli  IND     707\n",
      "8           Rohit Sharma  IND     704\n",
      "9        Kane Williamson   NZ     701\n"
     ]
    }
   ],
   "source": [
    "# answer 5 part B:\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "\n",
    "#extracting player names\n",
    "    pl_li=[]\n",
    "    \n",
    "    first_play1=soup.find('div',class_='rankings-block__banner--name')\n",
    "    first_play1=first_play1.get_text()\n",
    "    play_count=soup.find_all('td',class_='table-body__cell name')\n",
    "    \n",
    "    pl_li.append(first_play1)\n",
    "    for i in play_count:\n",
    "        pl_li.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "        \n",
    "#extracting team names\n",
    "    te_li=[]\n",
    "    \n",
    "    first_tn1=soup.find('div',class_='rankings-block__banner--nationality')\n",
    "    first_tn1=first_tn1.get_text().replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "    #print(first_tn1)\n",
    "    te_count=soup.find_all('span',class_='table-body__logo-text')\n",
    "    onlychar=\"\"\n",
    "    for char in first_tn1:\n",
    "        if char.isalpha():\n",
    "            onlychar+=char\n",
    "    \n",
    "    te_li.append(onlychar)\n",
    "    for i in te_count:\n",
    "        te_li.append(i.get_text())\n",
    "        \n",
    "#extracting rating\n",
    "    rat_li=[]\n",
    "    \n",
    "    first_count1=soup.find('div',class_='rankings-block__banner--rating')\n",
    "    first_count1=first_count1.get_text()\n",
    "    rat_count=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    \n",
    "    rat_li.append(first_count1)\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "        \n",
    "        \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"Player\"]=pl_li\n",
    "    df[\"team\"]=te_li\n",
    "    df[\"ratings\"]=rat_li\n",
    "    print(\"-------------Top 10 batsmen list----------\")\n",
    "    print(df.iloc[0:10,:])\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8794bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Top 10 bowlers list----------\n",
      "              Player team ratings\n",
      "0        Trent Boult   NZ     760\n",
      "1     Josh Hazlewood  AUS     727\n",
      "2     Mitchell Starc  AUS     665\n",
      "3     Shaheen Afridi  PAK     661\n",
      "4         Matt Henry   NZ     656\n",
      "5         Adam Zampa  AUS     655\n",
      "6       Mehedi Hasan  BAN     655\n",
      "7   Mujeeb Ur Rahman  AFG     650\n",
      "8  Mustafizur Rahman  BAN     640\n",
      "9        Rashid Khan  AFG     635\n"
     ]
    }
   ],
   "source": [
    "# answer 5 part C:\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "\n",
    "#extracting player names\n",
    "    pl_li=[]\n",
    "    new_li=[]\n",
    "    first_play1=[]\n",
    "    first_play1=soup.find_all('div',class_='rankings-block__banner--name')\n",
    "    for m in first_play1:\n",
    "        new_li.append(m.get_text())\n",
    "        \n",
    "    bwl_nam1=new_li[1]\n",
    "        \n",
    "   \n",
    "    play_count=soup.find_all('td',class_='table-body__cell name')\n",
    "    \n",
    "   # pl_li.append(first_play1)\n",
    "    for i in play_count:\n",
    "        pl_li.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "    \n",
    "    pl_li.insert(9,bwl_nam1)\n",
    "    \n",
    "     \n",
    "#extracting team names\n",
    "    te_li=[]\n",
    "    first_tn1=[]\n",
    "    bow_nat=[]\n",
    "    first_tn1=soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for m in first_tn1:\n",
    "        bow_nat.append(m.get_text().replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "    \n",
    "    bwl_nt1=bow_nat[1]\n",
    "    te_count=soup.find_all('span',class_='table-body__logo-text')\n",
    "    \n",
    "    onlychar=\"\"\n",
    "    for char in bwl_nt1:\n",
    "        if char.isalpha():\n",
    "            onlychar+=char\n",
    "    \n",
    "    #te_li.append(onlychar)\n",
    "    for i in te_count:\n",
    "        te_li.append(i.get_text())\n",
    "    \n",
    "    te_li.insert(9,onlychar)\n",
    "    \n",
    "#extracting rating\n",
    "    rat_li=[]\n",
    "    first_count1=[]\n",
    "    bowl_rat=[]\n",
    "    first_count1=soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for m in first_count1:\n",
    "        bowl_rat.append(m.get_text())\n",
    "        \n",
    "    bwl_rt1=bowl_rat[1]\n",
    "        \n",
    "    rat_count=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    \n",
    "    #rat_li.append(first_count1)\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "        \n",
    "    rat_li.insert(9,bwl_rt1)\n",
    "    \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df2=pd.DataFrame()\n",
    "    df[\"Player\"]=pl_li\n",
    "    df[\"team\"]=te_li\n",
    "    df[\"ratings\"]=rat_li\n",
    "    print(\"-------------Top 10 bowlers list----------\")\n",
    "    df2=df.iloc[9:19,:]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    print(df2)\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a149c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      team_names matches points  \\\n",
      "0      Australia      18  3,061   \n",
      "1   South Africa      26  3,098   \n",
      "2        England      25  2,904   \n",
      "3          India      27  2,820   \n",
      "4    New Zealand      24  2,425   \n",
      "5    West Indies      24  2,334   \n",
      "6     Bangladesh      12    932   \n",
      "7       Thailand       8    572   \n",
      "8       Pakistan      24  1,519   \n",
      "9      Sri Lanka       8    353   \n",
      "10       Ireland      14    548   \n",
      "\n",
      "                                              ratings  \n",
      "0   \\n                            170\\n           ...  \n",
      "1                                                 119  \n",
      "2                                                 116  \n",
      "3                                                 104  \n",
      "4                                                 101  \n",
      "5                                                  97  \n",
      "6                                                  78  \n",
      "7                                                  72  \n",
      "8                                                  63  \n",
      "9                                                  44  \n",
      "10                                                 39  \n"
     ]
    }
   ],
   "source": [
    "# answer 6 part A:\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "#extracting team names\n",
    "    mv_li=[]\n",
    "    team_names=soup.find_all('span',class_='u-hide-phablet')\n",
    "    for i in team_names:\n",
    "        mv_li.append(i.get_text())\n",
    "        \n",
    "    \n",
    "#extracting matches counts and points\n",
    "    mat_li=[]\n",
    "    mat_li2=[]\n",
    "    n=[]\n",
    "    p=[]\n",
    "    first_match=[]\n",
    "    first_match1=soup.find('td',class_='rankings-block__banner--matches')\n",
    "    first_match1=first_match1.get_text()\n",
    "    first_point1=soup.find('td',class_='rankings-block__banner--points')\n",
    "    first_point1=first_point1.get_text()\n",
    "    \n",
    "    mat_count=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    \n",
    "    mat_li.append(first_match1)\n",
    "    mat_li.append(first_point1)\n",
    "    \n",
    "    for i in mat_count:\n",
    "        mat_li.append(i.get_text())\n",
    "        \n",
    " \n",
    "   #n for storing first and all rest of the matches counts \n",
    "    k=0\n",
    "    for k in range(0,len(mat_li),2):\n",
    "        n.append(mat_li[k])\n",
    "\n",
    "        \n",
    "  #p for storing first and all rest of the matches points \n",
    "    y=0\n",
    "    for y in range(1,len(mat_li),2):\n",
    "        p.append(mat_li[y])\n",
    "    \n",
    "\n",
    "#extracting rating\n",
    "    rat_li=[]\n",
    "    \n",
    "    first_count1=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    first_count1=first_count1.get_text()\n",
    "    rat_count=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    \n",
    "    rat_li.append(first_count1)\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "        \n",
    "        \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"team_names\"]=mv_li\n",
    "    df[\"matches\"]=n\n",
    "    df[\"points\"]=p\n",
    "    df[\"ratings\"]=rat_li\n",
    "#print top 10 Womens teams\n",
    "    \n",
    "    print(df.iloc[0:11,:])\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31f9032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Top 10 women batsmen list----------\n",
      "                Player team ratings\n",
      "0         Alyssa Healy  AUS     785\n",
      "1          Beth Mooney  AUS     749\n",
      "2      Laura Wolvaardt   SA     732\n",
      "3       Natalie Sciver  ENG     725\n",
      "4     Harmanpreet Kaur  IND     716\n",
      "5      Smriti Mandhana  IND     714\n",
      "6          Meg Lanning  AUS     710\n",
      "7       Rachael Haynes  AUS     701\n",
      "8    Amy Satterthwaite   NZ     661\n",
      "9  Chamari Athapaththu   SL     655\n"
     ]
    }
   ],
   "source": [
    "# answer 6 part B:\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "\n",
    "#extracting player names\n",
    "    pl_li=[]\n",
    "    \n",
    "    first_play1=soup.find('div',class_='rankings-block__banner--name')\n",
    "    first_play1=first_play1.get_text()\n",
    "    play_count=soup.find_all('td',class_='table-body__cell name')\n",
    "    \n",
    "    pl_li.append(first_play1)\n",
    "    for i in play_count:\n",
    "        pl_li.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "        \n",
    "#extracting team names\n",
    "    te_li=[]\n",
    "    \n",
    "    first_tn1=soup.find('div',class_='rankings-block__banner--nationality')\n",
    "    first_tn1=first_tn1.get_text().replace(\" \",\"\").replace(\"\\n\",\"\")\n",
    "    #print(first_tn1)\n",
    "    te_count=soup.find_all('span',class_='table-body__logo-text')\n",
    "    onlychar=\"\"\n",
    "    for char in first_tn1:\n",
    "        if char.isalpha():\n",
    "            onlychar+=char\n",
    "    \n",
    "    te_li.append(onlychar)\n",
    "    for i in te_count:\n",
    "        te_li.append(i.get_text())\n",
    "        \n",
    "#extracting rating\n",
    "    rat_li=[]\n",
    "    \n",
    "    first_count1=soup.find('div',class_='rankings-block__banner--rating')\n",
    "    first_count1=first_count1.get_text()\n",
    "    rat_count=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    \n",
    "    rat_li.append(first_count1)\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "        \n",
    "        \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"Player\"]=pl_li\n",
    "    df[\"team\"]=te_li\n",
    "    df[\"ratings\"]=rat_li\n",
    "    print(\"-------------Top 10 women batsmen list----------\")\n",
    "    print(df.iloc[0:10,:])\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04652e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Top 10 all-rounder list----------\n",
      "             Player team ratings\n",
      "0   Hayley Matthews   WI     380\n",
      "1      Ellyse Perry  AUS     374\n",
      "2    Natalie Sciver  ENG     357\n",
      "3       Amelia Kerr   NZ     356\n",
      "4    Marizanne Kapp   SA     349\n",
      "5     Deepti Sharma  IND     322\n",
      "6  Ashleigh Gardner  AUS     270\n",
      "7     Jess Jonassen  AUS     246\n",
      "8    Jhulan Goswami  IND     214\n",
      "9   Katherine Brunt  ENG     207\n"
     ]
    }
   ],
   "source": [
    "# answer 6 part C:\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "\n",
    "#extracting player names\n",
    "    pl_li=[]\n",
    "    new_li=[]\n",
    "    first_play1=[]\n",
    "    first_play1=soup.find_all('div',class_='rankings-block__banner--name')\n",
    "    for m in first_play1:\n",
    "        new_li.append(m.get_text())\n",
    "    \n",
    "    bwl_nam1=new_li[-1]\n",
    "    \n",
    "    play_count=soup.find_all('td',class_='table-body__cell name')\n",
    "    \n",
    "   # pl_li.append(first_play1)\n",
    "    for i in play_count:\n",
    "        pl_li.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "    \n",
    "    pl_li.insert(18,bwl_nam1)\n",
    "    \n",
    "#extracting team names\n",
    "    te_li=[]\n",
    "    first_tn1=[]\n",
    "    bow_nat=[]\n",
    "    first_tn1=soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for m in first_tn1:\n",
    "        bow_nat.append(m.get_text().replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "    \n",
    "    bwl_nt1=bow_nat[-1]\n",
    "    te_count=soup.find_all('span',class_='table-body__logo-text')\n",
    "    \n",
    "    onlychar=\"\"\n",
    "    for char in bwl_nt1:\n",
    "        if char.isalpha():\n",
    "            onlychar+=char\n",
    "    \n",
    "    #te_li.append(onlychar)\n",
    "    for i in te_count:\n",
    "        te_li.append(i.get_text())\n",
    "    \n",
    "    te_li.insert(18,onlychar)\n",
    "    \n",
    "#extracting rating\n",
    "    rat_li=[]\n",
    "    first_count1=[]\n",
    "    bowl_rat=[]\n",
    "    first_count1=soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for m in first_count1:\n",
    "        bowl_rat.append(m.get_text())\n",
    "        \n",
    "    bwl_rt1=bowl_rat[-1]\n",
    "        \n",
    "    rat_count=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    \n",
    "    #rat_li.append(first_count1)\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "        \n",
    "    rat_li.insert(18,bwl_rt1)\n",
    "    \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df2=pd.DataFrame()\n",
    "    df[\"Player\"]=pl_li\n",
    "    df[\"team\"]=te_li\n",
    "    df[\"ratings\"]=rat_li\n",
    "    print(\"-------------Top 10 all-rounder list----------\")\n",
    "    df2=df.iloc[18:,:]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    print(df2)\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "358c1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Headline----------------\n",
      "['Fed Chair Powell says smaller interest rate hikes could start in December']\n",
      "----------Time---------------\n",
      "['Published Wed, Nov 30 20221:30 PM EST', 'Updated 1 Min Ago']\n",
      "----------NewsLink---------------\n",
      "['/live-tv/']\n"
     ]
    }
   ],
   "source": [
    "#answer7\n",
    "\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "#extracting Headline \n",
    "    Hd_li=[]\n",
    "    head_lines=[]\n",
    "    head_lines=soup.find_all('h1',class_='ArticleHeader-headline')   \n",
    "    for i in head_lines:\n",
    "        Hd_li.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "    print(\"----------Headline----------------\")\n",
    "    print(Hd_li)\n",
    "    \n",
    "#extracting time \n",
    "    Hd_ti=[]\n",
    "    head_t=[]\n",
    "    head_t=soup.find_all('time')  \n",
    "    #print(head_t)\n",
    "    for i in head_t:\n",
    "        Hd_ti.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "    print(\"----------Time---------------\")\n",
    "    print(Hd_ti)    \n",
    "\n",
    "#extracting newlink \n",
    "    news_ti=[]\n",
    "    n_t=[]\n",
    "    n_t=soup.find_all('a',class_='WatchLive-container')  \n",
    "    \n",
    "    for i in n_t:\n",
    "        news_ti.append(i.get('href'))\n",
    "    print(\"----------NewsLink---------------\")\n",
    "    print(news_ti)   \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.cnbc.com/2022/11/30/fed-chair-jerome-powell-says-smaller-rate-hikes-could-come-in-december.html\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d86cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Top downlaoded journal list----------\n",
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1                           Making sense of raw input   \n",
      "2   Law and logic: A review from an argumentation ...   \n",
      "3              Creativity and artificial intelligence   \n",
      "4   Artificial cognition for social human–robot in...   \n",
      "5   Explanation in artificial intelligence: Insigh...   \n",
      "6                       Making sense of sensory input   \n",
      "7   Conflict-based search for optimal multi-agent ...   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...   \n",
      "9   The Hanabi challenge: A new frontier for AI re...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11           Argumentation in artificial intelligence   \n",
      "12  Algorithms for computing strategies in two-pla...   \n",
      "13      Multiple object tracking: A literature review   \n",
      "14  Selection of relevant features and examples in...   \n",
      "15  A survey of inverse reinforcement learning: Ch...   \n",
      "16  Explaining individual predictions when feature...   \n",
      "17  A review of possible effects of cognitive bias...   \n",
      "18  Integrating social power into the decision-mak...   \n",
      "19  “That's (not) the output I expected!” On the r...   \n",
      "20  Explaining black-box classifiers using post-ho...   \n",
      "21  Algorithm runtime prediction: Methods & evalua...   \n",
      "22              Wrappers for feature subset selection   \n",
      "23  Commonsense visual sensemaking for autonomous ...   \n",
      "24         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "2                    Prakken, Henry, Sartor, Giovanni    October 2015   \n",
      "3                                  Boden, Margaret A.     August 1998   \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "5                                         Miller, Tim   February 2019   \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "11                Bench-Capon, T.J.M., Dunne, Paul E.    October 2007   \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "14                       Blum, Avrim L., Langley, Pat   December 1997   \n",
      "15                    Arora, Saurabh, Doshi, Prashant     August 2021   \n",
      "16       Aas, Kjersti, Jullum, Martin, Løland, Anders  September 2021   \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "18     Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.   December 2016   \n",
      "19                       Riveiro, Maria, Thill, Serge  September 2021   \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "22                       Kohavi, Ron, John, George H.   December 1997   \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "24                                    Ying, Mingsheng   February 2010   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "#answer8\n",
    "\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "#extracting journal name \n",
    "    Hd_li=[]\n",
    "    head_lines=[]\n",
    "    head_lines=soup.find_all('h2',class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg')   \n",
    "    for i in head_lines:\n",
    "        Hd_li.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "#extracting authornames \n",
    "    Hd_ti=[]\n",
    "    head_t=[]\n",
    "    head_t=soup.find_all('span',class_='sc-1w3fpd7-0 dnCnAO')  \n",
    "    #print(head_t)\n",
    "    for i in head_t:\n",
    "        Hd_ti.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "  \n",
    "   \n",
    "#extracting published date\n",
    "    news_ti=[]\n",
    "    n_t=[]\n",
    "    n_t=soup.find_all('span',class_='sc-1thf9ly-2 dvggWt')  \n",
    "    \n",
    "    for i in n_t:\n",
    "        news_ti.append(i.get_text().replace(\"\\n     \",\"\").replace(\"\\n\",\"\").strip())\n",
    "    \n",
    "    \n",
    "#extracting reward URL date\n",
    "    url_ti=[]\n",
    "    lin=[]\n",
    "    lin=soup.find_all('a',class_='sc-5smygv-0 fIXTHm')  \n",
    "    for i in lin:\n",
    "        url_ti.append(i.get('href'))\n",
    "        \n",
    "    \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"Paper Title\"]=Hd_li\n",
    "    df[\"Authors\"]=Hd_ti\n",
    "    df[\"Published Date\"]=news_ti\n",
    "    df[\"Paper URL\"]=url_ti\n",
    "    print(\"-------------Top downlaoded journal list----------\")\n",
    "    print(df)\n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c78000c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Dine out details----------\n",
      "                     Restaurant name  \\\n",
      "0                            Tamasha   \n",
      "1                              Local   \n",
      "2                        Station Bar   \n",
      "3                   Ministry Of Beer   \n",
      "4                                QBA   \n",
      "5                Unplugged Courtyard   \n",
      "6                  The Junkyard Cafe   \n",
      "7                 Cafe Delhi Heights   \n",
      "8                      The G.T. ROAD   \n",
      "9         The Luggage Room By Sandoz   \n",
      "10                     My Bar Square   \n",
      "11                       Dasaprakash   \n",
      "12                             Chido   \n",
      "13   Ardor 2.1 Restaurant and Lounge   \n",
      "14               My Bar Headquarters   \n",
      "15        Somewhere Restaurant & Bar   \n",
      "16             Dhaba Estd 1986 Delhi   \n",
      "17  Connaught Clubhouse Microbrewery   \n",
      "18                            Sandoz   \n",
      "19          Out Of The Box Courtyard   \n",
      "20                    Openhouse Cafe   \n",
      "\n",
      "                                              Cuisine  \\\n",
      "0           Continental, Asian, Italian, North Indian   \n",
      "1                    North Indian, Asian, Continental   \n",
      "2           Italian, Chinese, North Indian, Fast Food   \n",
      "3          North Indian, Continental, American, Asian   \n",
      "4                  North Indian, Continental, Italian   \n",
      "5    North Indian, Italian, Chinese, Turkish, Cont...   \n",
      "6       North Indian, Continental, Chinese, Fast Food   \n",
      "7    Continental, North Indian, Beverages, Chinese...   \n",
      "8                                        North Indian   \n",
      "9         Chinese, Italian, North Indian, Continental   \n",
      "10         Finger Food, Chinese, Continental, Italian   \n",
      "11   North Indian, South Indian, Beverages, Chines...   \n",
      "12   North Indian, Italian, Continental, Asian, Fi...   \n",
      "13        North Indian, Chinese, Italian, Continental   \n",
      "14                              North Indian, Chinese   \n",
      "15                   North Indian, Continental, Asian   \n",
      "16                              North Indian, Mughlai   \n",
      "17          North Indian, Continental, Asian, Chinese   \n",
      "18                 North Indian, Chinese, Continental   \n",
      "19      North Indian, Mediterranean, Chinese, Italian   \n",
      "20                       North Indian, Asian, Italian   \n",
      "\n",
      "                                        Location Ratings  \\\n",
      "0                 Connaught Place, Central Delhi     4.2   \n",
      "1   Scindia House,Connaught Place, Central Delhi       4   \n",
      "2         F-Block,Connaught Place, Central Delhi       4   \n",
      "3         M-Block,Connaught Place, Central Delhi       4   \n",
      "4                 Connaught Place, Central Delhi     4.3   \n",
      "5                 Connaught Place, Central Delhi       4   \n",
      "6                 Connaught Place, Central Delhi     4.1   \n",
      "7                         Janpath, Central Delhi     4.3   \n",
      "8         M-Block,Connaught Place, Central Delhi     4.3   \n",
      "9         M-Block,Connaught Place, Central Delhi     3.9   \n",
      "10                Connaught Place, Central Delhi     3.9   \n",
      "11                Connaught Place, Central Delhi     4.2   \n",
      "12                Connaught Place, Central Delhi     4.2   \n",
      "13                Connaught Place, Central Delhi     3.8   \n",
      "14                Connaught Place, Central Delhi       4   \n",
      "15                Connaught Place, Central Delhi     4.1   \n",
      "16                Connaught Place, Central Delhi     4.1   \n",
      "17                Connaught Place, Central Delhi     4.3   \n",
      "18                Connaught Place, Central Delhi       4   \n",
      "19                Connaught Place, Central Delhi     4.1   \n",
      "20                Connaught Place, Central Delhi     4.1   \n",
      "\n",
      "                                            Image URL  \n",
      "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "20  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "# answer 9:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "#extracting Restaurant name\n",
    "    rl_li=[]\n",
    "    R_count=[]\n",
    "    R_count=soup.find_all('a',class_='restnt-name ellipsis')\n",
    "   \n",
    "    for i in R_count:\n",
    "        rl_li.append(i.get_text())\n",
    "        \n",
    "          \n",
    "#extracting Cuisine\n",
    "\n",
    "    te_li=[]\n",
    "    cus=[]\n",
    "    \n",
    "    te_count=soup.find_all('span',class_='double-line-ellipsis')\n",
    "    for i in te_count:\n",
    "        te_li.append(i.get_text().split(\"|\")[1])\n",
    "\n",
    "    \n",
    "#extracting Location\n",
    "    rat_li=[]\n",
    "    rat_count=[]\n",
    "    \n",
    "    rat_count=soup.find_all('div',class_='restnt-loc ellipsis')\n",
    "\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "    \n",
    "    #print(rat_li)  \n",
    "\n",
    "#extracting Ratings\n",
    "    med_li=[]\n",
    "    med_count=[]\n",
    "    \n",
    "    med_count=soup.find_all('div',class_='restnt-rating rating-4')\n",
    "\n",
    "    for i in med_count:\n",
    "        med_li.append(i.get_text())\n",
    "    \n",
    "#extracting Image URL\n",
    "    ed_li=[]\n",
    "    ed_count=[]\n",
    "    \n",
    "    ed_count=soup.find_all('img',class_='no-img')\n",
    "    \n",
    "    for i in ed_count:\n",
    "        ed_li.append(i.get('data-src'))\n",
    "    \n",
    "    #print(ed_li)\n",
    "\n",
    "        \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df[\"Restaurant name\"]=rl_li\n",
    "    df[\"Cuisine\"]=te_li\n",
    "    df[\"Location\"]=rat_li\n",
    "    df[\"Ratings\"]=med_li\n",
    "    df[\"Image URL\"]=ed_li\n",
    "    \n",
    "    print(\"-------------Dine out details----------\")\n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://www.dineout.co.in/delhi-restaurants?search_str=delhi\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4620e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Top publication list----------\n",
      "                                            Publication h5-index h5-median\n",
      "Rank                                                                      \n",
      "1.                                               Nature      444       667\n",
      "2.                  The New England Journal of Medicine      432       780\n",
      "3.                                              Science      401       614\n",
      "4.    IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
      "5.                                           The Lancet      354       635\n",
      "...                                                 ...      ...       ...\n",
      "96.                        Journal of Business Research      145       233\n",
      "97.                                    Molecular Cancer      145       209\n",
      "98.                                             Sensors      145       201\n",
      "99.                               Nature Climate Change      144       228\n",
      "100.                    IEEE Internet of Things Journal      144       212\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# answer 10:\n",
    "import pandas as pd\n",
    "def webscr1(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "\n",
    "#extracting Rank \n",
    "    pl_li=[]=[]\n",
    "    Rank_count=[]\n",
    "    Rank_count=soup.find_all('td',class_='gsc_mvt_p')\n",
    "\n",
    "    for i in Rank_count:\n",
    "        pl_li.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "        \n",
    "  \n",
    "        \n",
    "#extracting Publication\n",
    "\n",
    "    te_li=[]\n",
    "    \n",
    "    te_count=soup.find_all('td',class_='gsc_mvt_t')\n",
    "    for i in te_count:\n",
    "        te_li.append(i.get_text())\n",
    "        \n",
    "    \n",
    "#extracting h5-index\n",
    "    rat_li=[]\n",
    "    rat_count=[]\n",
    "    \n",
    "    rat_count=soup.find_all('a',class_='gs_ibl gsc_mp_anchor')\n",
    "\n",
    "    for i in rat_count:\n",
    "        rat_li.append(i.get_text())\n",
    "    \n",
    "\n",
    "\n",
    "#extracting h5-median\n",
    "    med_li=[]\n",
    "    med_count=[]\n",
    "    \n",
    "    med_count=soup.find_all('span',class_='gs_ibl gsc_mp_anchor')\n",
    "\n",
    "    for i in med_count:\n",
    "        med_li.append(i.get_text())\n",
    "    \n",
    "        \n",
    "#storing values in dataframe    \n",
    "    df=pd.DataFrame()\n",
    "    df2=pd.DataFrame()\n",
    "    df[\"Rank\"]=pl_li\n",
    "    df[\"Publication\"]=te_li\n",
    "    df[\"h5-index\"]=rat_li\n",
    "    df[\"h5-median\"]=med_li\n",
    "    \n",
    "    print(\"-------------Top publication list----------\")\n",
    "    df2=df\n",
    "    #df2.reset_index(drop=True, inplace=TRUE, index={\"\"})\n",
    "    df2.set_index(['Rank'], inplace = True)\n",
    "    print(df2)\n",
    "    \n",
    "    \n",
    "#calling function   \n",
    "url=\"https://scholar.google.com/citations?view_op=top_venues&hl=en\"\n",
    "webscr1(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfc647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
